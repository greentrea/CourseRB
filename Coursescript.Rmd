---
title: "2024 UZH R-Bridge Workshop"
author: "Stefan Graf & Saskia Stierli"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Script description
This is an R Markdown for the workshop "Linking R with Web Mapping: The R-ArcGIS Bridge" at the Unviersity of Zurich.


## Setup R for workshop
Setting up your R environment.

```{r Setup, echo=TRUE, eval = FALSE}
# CLASSICAL PACKAGE IMPORTERs

# Clean environment if necessary
# rm(list = ls())


packages_checker <- function(packages_list){
  for (package in packages_list){
    if (!require(package, character.only = T)) {
      install.packages(package)
    }
    library(package, character.only = T)
  }
}
packages_checker(
  c('dplyr',
    'arcgis',
    'sf',
    'ggplot2',
    #'GGally',
    'tidyverse',
    'rmarkdown',
    'knitr'
    #'cli' # last week something didnt work with progress bars
    ))



renv::restore()

# See working directory
getwd()

# Usefull shortcuts on windows
# set variable [alt+-]
# piping [ctrl+shift+m]
# format code [ctrl+l]
# uncomment or comment code [ctrl+shift+c]



```

## An example of how to use the ArcGIS Bridge (Demo)
### Demo access public living atlas data

You can load public services from the living atlas without licensing or use any ArcGIS License. Feature layers will be read in as [sf](https://r-spatial.github.io/sf/) object. See the [living atlas](https://livingatlas.arcgis.com/en/home/) of Esri for "official" datasets hosted by Esri or just search [ArcGIS Online (Sign in)](https://www.arcgis.com/home/) for any publicly available dataset. :

```{r Demo access public living atlas data, echo=TRUE, eval=FALSE}
# Save url of Service
accidents_url <- 'https://services9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/Polizeilich_registrierte_Verkehrsunfälle_im_Kanton_Zürich_seit_2011/FeatureServer/0' # feature service of all traffic accidents of the canton of Zurich.

# Connect to feature service
accidents_featurelayer <- arc_open(accidents_url) # meta data appears in R environment data as a list
# Add optionally a filter with arc_open(layername, where = "AccidentSevirety='fatal'")

# Create a SF object from feature service
accidents_featurelayer_sf <- arc_select(accidents_featurelayer)

# Plot traffic accidents by severity
ggplot(data = accidents_featurelayer_sf) +
  geom_sf(aes(color = AccidentSeverityCategory), size = 0.5, alpha = 0.6) +
  scale_color_viridis_d(option = "C", end = 0.8) + # Option C provides a varied color scale for severity
  labs(
    title = "Traffic Accidents by Severity in the Canton of Zurich",
    color = "Severity Category"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

# #Or load a huge dataset ;) just takes 12h (1 Mio is ok but if its above it gets slow like this 100 Mio Dataset)
# iNaturalist_url <- 'https://services9.arcgis.com/RHVPKKiFTONKtxq3/arcgis/rest/services/iNat_PreUC_View/FeatureServer/0' # feature service of all traffic accidents of the canton of Zurich.
# iNaturalist_fl <- arc_open(iNaturalist_url) # meta data appears in R environment data as a list
# iNaturalist_sf <- arc_select(iNaturalist_fl,
#                              where = "scientific_name ='pongo abelii'") # Filter f.e. for Orangutans

```
### Demo access private data of your account or publish data in your account (ArcGIS Online or ArcGIS Enterprise)


```{r Demo access private data, echo=TRUE, eval=FALSE}
## Access private data of my ArcGIS Account
# Save url of Service
sruveyresults_url <- 'https://services9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/survey123_111a8cddbd3c494bb25b685d6984631f_results/FeatureServer/0'

# Connect to feature service 
sruveyresults_fl <- arc_open(sruveyresults_url) # --> We cant connect as this data is private of my account! We first have to authorize R to access.


```

#### Authorization Process to connect R to my ArcGIS Account (hold your ArcGIS Login ready)
There a number of times when you will need to verify who you say you are when using ArcGIS location services. This is done through a process called authentication. Authentication is required when you want to:

*access private content
*publish your own layers
*use services like geocode addresses in bulk from an ArcGIS client service
*store geocoding results in a file
*or access POI data through {arcgisplaces}

If you want to use R within ArcGIS Pro see the arcgisbinding package authorization [here](https://r.esri.com/r-bridge-site/docs/installation.html)


#### Now a short tricky bit! ;). - Our Authentication process
There are some options to "login" or authenticate ArcGIS online. Originally there were just functions to put in your password and credentials. After years some standards were established and we will for this course now use the following:
The code flow-based authentication in ArcGIS for R uses OAuth2. When you run auth_code(), a browser tab opens, prompting you to sign in and copy an authorization code. Paste this code back into R to generate a temporary access token, allowing the application to interact with your account securely. This token expires periodically, so you’ll need to renew it as needed. For further details, you can view the full guide here (https://r.esri.com/r-bridge-site/docs/auth/connecting-to-a-portal.html).

#### 
If a client ID is not provided to you by an administrator and you have the ability to create content items, you can create one.

You can do so by creating an application item.

*Log in to your ArcGIS Online or ArcGIS Enterprise organization
*Navigate to the Content tab
*Click New Item
*Select Application
*Choose Other application as your Application Type
*Give the item an informative name such as r-arcgis
*Optionally, specify the folder, tags, and summary as well.
*You will be directed to the item details page of the newly created application where you can see your credentials. Do not share these.

```{r Authorization, echo=TRUE, eval=FALSE}

# Obtaining a Client ID
# Please follow these steps see above https://r.esri.com/r-bridge-site/docs/auth/connecting-to-a-portal.html#obtaining-a-client-id

# Authorization there are different methods all a bit complicated! I choose the easiest way. Please run the below seperately and login in the opening browser.
Sys.setenv(ARCGIS_CLIENT = "qhmvug4Qc984TWRV") #Use your own Client ID
token <- auth_code() # Allow in browser and enter token in console!
set_arc_token(token) # set token for all future environment functions (but if you get strange error messages while opening private feature layer --> add token manually to function)
# old way set_arc_token(auth_user()) --> if sth doesnt work test this
```

#### Demo - Reading private vector or image layers (types of AGOL layers)

```{r reading private, echo=TRUE, eval=FALSE}
# Load survey results from my private survey with you
survey_url <- "https://services9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/survey123_a26158c869224447903aa8dad3029715_results/FeatureServer/0"

# Read in as a reference layer
survey_fl <- arc_open(survey_url) # token needed if not public layer

# Read in as SF object
survey_sf <- arc_select(survey_fl)

# # Its also possible to read in imagery services
# url <- "https://tiledimageservices9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/Z%C3%BCrich_Swissimage_Test_RBridge/ImageServer"
# worldimagery_esrimap_url_img <- arc_open(url,token = token)
# 
# worldimagery_esrimap_url_img$extent
# zurich <- arc_raster(
#   x = worldimagery_esrimap_url,
#   xmin = "948917.1",   # Numeric value
#   xmax = "951906.2",    # Numeric value
#   ymin = "6002429",   # Numeric value
#   ymax = "6005428"    # Numeric value
# )
# crater
# terra::plotRGB(crater, stretch = "lin")

```

#### Demo Exkursus - Reading attachments from a survey

```{r Exkursus reading survey attachments, echo=TRUE, eval=FALSE}

## Exursus, you can also load attachments in a folder from a survey

##### Demo Survey123 ######
print("will come soon")

```


#### Demo - Using ArcGIS Services

```{r geocode, echo=TRUE, eval=FALSE}
# Geocoding = words --> Location, reverse geocoding = location --> words
###########################################
# Read in "Take aways" prepared by me in ArcGIS Online
takeaways_url <- "https://services9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/Just_eat_restaurants_in_Zurich/FeatureServer/0"

# Read in as a reference layer
takeaways_fl <- arc_open(takeaways_url)

# Read in as SF object
takeaways_sf <- arc_select(takeaways_fl)

###########################################
# Read bars API from the City of Zurich
# Load JSON data from the URL
url <- "https://www.zuerich.com/en/api/v2/data?id=103"
json_data <- fromJSON(url, flatten = TRUE)

# Convert JSON to a dataframe
bars_df <- as.data.frame(json_data)

# Cleanup
# Clean up the data frame
bars_df <- bars_df %>%
  
  # Step 1: Select and rename important columns to put them at the front
  select(
    name = name.en,          # Rename as needed
    name_de = name.de,
    type = `@type`, 
    id = identifier, 
    photo = photo, 
    opening_hours = openingHours, 
    opening_hours_spec = openingHoursSpecification, 
    place = place,
    
    # Step 2: Select other relevant columns
    everything()
  ) %>%
  
  # Step 3: Rename all remaining column names to lowercase and underscores
  rename_with(~ tolower(gsub("\\.", "_", .x))) 


# Add a unique ID to the original bars_df for tracking
bars_df <- bars_df %>%
  mutate(original_id = row_number())

# Create a subset of bars_df with missing coordinates
bars_df_nocoords <- bars_df %>% 
  filter(is.na(geocoordinates_latitude)) %>%
  mutate(full_address = paste(address_streetaddress, address_postalcode, address_addresscountry, sep = ", "))

# Perform geocoding
geocoded <- geocode_addresses(single_line = bars_df_nocoords[["full_address"]])

# Extract latitude and longitude from geometry if available
if ("geometry" %in% colnames(geocoded)) {
  geocoded_coords <- st_coordinates(geocoded$geometry)
  geocoded <- geocoded %>%
    mutate(
      latitude = geocoded_coords[, 2],
      longitude = geocoded_coords[, 1]
    )
} else {
  warning("No geometry found in geocoded results.")
}

# Bind geocoded results back to bars_df_nocoords, keeping original identifiers
bars_df_nocoords <- bars_df_nocoords %>%
  bind_cols(
    latitude = geocoded$latitude,
    longitude = geocoded$longitude
  )

# Merge back into the original bars_df using the original ID
bars_df <- bars_df %>%
  left_join(
    bars_df_nocoords %>% select(original_id, latitude, longitude),
    by = c("original_id" = "original_id")
  ) %>%
  mutate(
    geocoordinates_latitude = ifelse(is.na(geocoordinates_latitude), latitude, geocoordinates_latitude),
    geocoordinates_longitude = ifelse(is.na(geocoordinates_longitude), longitude, geocoordinates_longitude)
  ) %>%
  select(-latitude, -longitude)

# Convert to sf object
bars_sf <- st_as_sf(bars_df, coords = c("geocoordinates_longitude", "geocoordinates_latitude"), crs = 4326)


```

#### Demo - Publishing from R to AGOL 

```{r publish, echo=TRUE, eval=FALSE}
# publish a sf object
bars_sf <- bars_sf %>%
  mutate(
    tomasbookingid = sapply(tomasbookingid, as.character), # Convert logical to character
    zurichcard = sapply(zurichcard, as.character), # Convert logical to character
    place = sapply(place, function(x) if (is.list(x)) paste(x, collapse = "; ") else as.character(x)),
    opens = sapply(opens, function(x) if (is.list(x)) paste(x, collapse = "; ") else as.character(x))
  )


bars_sf <- bars_sf %>% 
  mutate(
    photo = map_chr(photo, ~ paste(unlist(.), collapse = "; ")),
    opening_hours = map_chr(opening_hours, ~ paste(unlist(.), collapse = "; ")),
    opening_hours_spec = map_chr(opening_hours_spec, ~ paste(unlist(.), collapse = "; ")),
    place = map_chr(place, ~ paste(unlist(.), collapse = "; ")),
    opens = map_chr(opens, ~ paste(unlist(.), collapse = "; ")),
    detailedinformation_de = map_chr(detailedinformation_de, ~ paste(unlist(.), collapse = "; ")),
    detailedinformation_en = map_chr(detailedinformation_en, ~ paste(unlist(.), collapse = "; ")),
    detailedinformation_fr = map_chr(detailedinformation_fr, ~ paste(unlist(.), collapse = "; ")),
    detailedinformation_it = map_chr(detailedinformation_it, ~ paste(unlist(.), collapse = "; "))
  )
bars_sf <- bars_sf %>% 
  rename(context = `@context`, customtype = `@customtype`)


# Define the important columns including only _de and _en columns
important_columns <- c("name", "type", "id", "photo", "opening_hours", 
                       "opening_hours_spec", "context", "customtype", 
                       "license", "tomasbookingid", "zurichcard", 
                       "osm_id", "datemodified", "opens", 
                       "copyrightholder_de", "copyrightholder_en", 
                       "name_de",
                       "disambiguatingdescription_de", "disambiguatingdescription_en", 
                       "description_de", "description_en", 
                       "image_url")

# Remove HTML tags and entities from the description_de column
bars_sf_important$description_de <- gsub("<[^>]+>", "", bars_sf_important$description_de)  # Remove HTML tags
bars_sf_important$description_de <- str_replace_all(bars_sf_important$description_de, "&[a-zA-Z]+;", "")  # Remove HTML entities

# If you're also concerned about the description_en column, apply the same cleaning
bars_sf_important$description_en <- gsub("<[^>]+>", "", bars_sf_important$description_en)  # Remove HTML tags
bars_sf_important$description_en <- str_replace_all(bars_sf_important$description_en, "&[a-zA-Z]+;", "")  # Remove HTML entities

# Create a new data frame with only the important columns
bars_sf_important <- bars_sf[ , important_columns]


# Publish the new data frame
arcgislayers::publish_layer(bars_sf_important, 
                            title = "Bars in Zurich test", token = token)

```

#### Demo - Editing AGOL layers directly in R
We can also alter already published layer or empty and refill a published layer (similar as delete but without change of naming, and attribute schema). Therefore the following functions can be used.
*truncate_layer()
*add_features()
*update_features()
*delete_features()

```{r editing, echo=TRUE, eval=FALSE}
### Republish full layer (truncate and add_features)
# Oh we have some additional items we wanna add. Or we want to replace the whole spatial data frame (sf object). So lets truncate or republish the layer (this is not a full deletion as this is not possible but we empty the feature service and refill it.)
bars_online_url <- "https://services9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/Bars%20in%20Zurich/FeatureServer/0"

bars_online_fl <- arc_open(bars_online_url)

bars_online_fl[["supportsTruncate"]]

truncate_res <- truncate_layer(bars_online_fl)

bars_online_fl <- refresh_layer(bars_online_fl)

# Check for NA or empty strings, or other stuff which causes errors in add_features
bars_sf_important <- bars_sf_important[!is.na(bars_sf_important$description_en) & bars_sf_important$description_en != "", ]
bars_sf_important <- bars_sf_important[!is.na(bars_sf_important$description_de) & bars_sf_important$description_de != "", ]
bars_sf_important <- bars_sf_important %>% select(-description_de, -description_en) 

# Proceed to add features
add_res <- add_features(bars_online_fl, bars_sf_important)

bars_online_fl <- refresh_layer(bars_online_fl) # if you use it in R again


### Other possible edit functions:
# Alternatively, we can delete features based on a where clause. Say we wanted to delete all of the features where the zurichcard value is true. We can accomplish this using a where clause.

bars_online_url_deletion_url <- "https://services9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/Bars%20in%20Zurich/FeatureServer/0"
bars_online_deletion <- arc_open(bars_online_url_deletion_url)
bars_online_deleted <- delete_features(bars_online_deletion, where = "object_id < 100")
bars_online_deleted # See which are deleted, also check in AGOL if layer did really refresh and deletion is complete.

```




## Exercises
### Exercises preparation

```{r exercise preparation, echo=FALSE, eval=FALSE}

# Obtaining a Client ID
# Please follow these steps see above https://r.esri.com/r-bridge-site/docs/auth/connecting-to-a-portal.html#obtaining-a-client-id

# Authorization there are different methods all a bit complicated! I choose the easiest way. Please run the below seperately and login in the opening browser.
Sys.setenv(ARCGIS_CLIENT = "qhmvug4Qc984TWRV") #Use your own Client ID
token <- auth_code() # Allow in browser and enter token in console!
set_arc_token(token) # set token for all future environment functions (but if you get strange error messages while opening private feature layer --> add token manually to function)
# old way set_arc_token(auth_user()) --> if sth doesnt work test this

```

### Task 1

```{r}
## load public data from arcgis online (I uploaded it there ;))

# Save url of Service
url_Zurich <- 'https://services9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/AirBnB_Prices_Zurich_2017/FeatureServer/0'

# Connect to feature service
Zurich_featurelayer <- arc_open(url_Zurich) # meta data appears in R environment data as a list
# Add optionally a filter with arc_open(layername, where = "AccidentSevirety='fatal'")

# Create a SF object from feature service
df_Zurich <- arc_select(Zurich_featurelayer)

# alternative if you have it locally ;) df_Zurich <- read.csv("data/zurich/s3_files/zurich/tomslee_airbnb_zurich_1363_2017-06-22.csv")
```

```{r}
## Check for zeros or NA values in price and bedrooms and keep only valid rows
df_Zurich <- df_Zurich[!is.na(df_Zurich$price) & !is.na(df_Zurich$bedrooms) & df_Zurich$bedrooms > 0, ]
```

```{r}
## Linear regression to determine the correlation between price and bedrooms
model <- lm(price ~ bedrooms, data = df_Zurich)
summary(model)

## Predicting prices based on the model
df_Zurich$predicted_price <- predict(model, newdata = df_Zurich)

## Plot of actual and predicted prices
plot(df_Zurich$bedrooms, df_Zurich$price, main = "Price vs. Sleepingrooms")
abline(model, col = "blue")  # Adds the trend line
```
```{r}
## Determine slope
slope <- coef(model)["bedrooms"]

## Calculate new attribute: price per bedroom
df_Zurich$price_per_bedroom <- df_Zurich$price / (df_Zurich$bedrooms * slope)

# Values above 1 indicate a price higher than expected, and values below 1 indicate a lower-than-expected price based on the model's slope.
```

```{r}
## Calculating the 5th and 95th percentiles for price_per_bedroom
lower_bound <- quantile(df_Zurich$price_per_bedroom, 0.05, na.rm = TRUE)
upper_bound <- quantile(df_Zurich$price_per_bedroom, 0.95, na.rm = TRUE)

## Remove values outside the 5% and 95% limits
df_Zurich_no_outliers <- df_Zurich[df_Zurich$price_per_bedroom >= lower_bound & df_Zurich$price_per_bedroom <= upper_bound, ]
```

```{r}
## Calculate minimum and maximum for normalization
min_price_per_bedroom <- min(df_Zurich_no_outliers$price_per_bedroom, na.rm = TRUE)
max_price_per_bedroom <- max(df_Zurich_no_outliers$price_per_bedroom, na.rm = TRUE)
```

```{r}
## Normalization: normalize price per bedroom to the range [0, 1]
df_Zurich_no_outliers$normalized_price_per_bedroom <- (df_Zurich_no_outliers$price_per_bedroom - min_price_per_bedroom) / 
                                                      (max_price_per_bedroom - min_price_per_bedroom)

```

```{r}
## Publish the layer to ArcGIS Online
df_Zurich_no_outliers<- df_Zurich_no_outliers %>% dplyr::select(-bathrooms,-minstay) # bathrooms is empty

arcgislayers::publish_layer(
  df_Zurich_no_outliers,
  title = "AirBnB Prices Zurich points", token = token)

```
### Task 2

```{r}
## Import the feature layer of the city districts from ArcGIS Online via URL below (I provide this). Replace the URL of the zurich aribnb point data with your published dataset url.
# Remember to add the /0 at the end of the link. Also check there are no spaces allowed replace them with %
stadtkreise <- arc_select(arc_open("https://services9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/stzh_adm_stadtkreise_a/FeatureServer/0"))
zurich_airbnb_sf <- arc_select(arc_open("https://services9.arcgis.com/Va7wxtFE2VgOj6pj/arcgis/rest/services/AirBnB%20Prices%20Zurich%20points/FeatureServer/0"))

# plot the stadtkreise
ggplot(data = stadtkreise) +
  geom_sf() +
  theme_minimal() +
  ggtitle("Stadtkreise Zürich")

```

```{r}
# Spatial linking of Airbnb data with urban districts
price_per_district <- st_join(zurich_airbnb_sf, stadtkreise, join = st_within)

# Calculate the average price per urban district
average_price <- as.data.frame(price_per_district) %>% 
  select(-geometry) %>% 
  group_by(neighborhood) %>%
  summarise(avg_price = mean(price, na.rm = TRUE))

```

```{r}
# Convert the urban district SF object to a data frame
stadtkreise_df <- as.data.frame(stadtkreise)

# Adding the average price to the urban districts
stadtkreise_df <- stadtkreise_df %>%
  left_join(average_price, by = c("bezeichnung" = "neighborhood")) 

# Convert it back to an sf object.
stadtkreise_prices <- st_as_sf(stadtkreise_df)

```

```{r}
## Publish the polygon layer to ArcGIS Online
arcgislayers::publish_layer(
  stadtkreise_prices,
  title = "Avg Prices per District Zurich Airbnb",
  description = "RBridge Course: Avg Prices per District Zurich Airbnb")

```

```{r}
# Optional: Task three you go to ArcGIS online to look at the dataset below and compare it with your published layer. I already published it for you.
zuri_complete <- read_csv("data/perception/zuri_complete.csv")

```
